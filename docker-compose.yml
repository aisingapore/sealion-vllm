services:
  sealion-vllm:
    image: vllm/vllm-openai:v0.6.3
    container_name: sealion-vllm
    environment:
      # Disable the NVIDIA constraints
      # Remove this environment variable if the environment satisfies the constraints
      - NVIDIA_DISABLE_REQUIRE=true
    ports:
      - "8000:8000"
    restart: no
    volumes:
      - ./models/${MODEL_NAME:-Llama-SEA-LION-v3-8B-IT}:/models/${MODEL_NAME:-Llama-SEA-LION-v3-8B-IT}
    deploy:
      resources:
        reservations:
          devices:
            - capabilities: ["gpu"]
    command: [
      "--dtype", "half",
      "--max-model-len", "512",
      "--enforce-eager",
      "--served-model-name", "${MODEL_NAME:-Llama-SEA-LION-v3-8B-IT}",
      "--model", "/models/${MODEL_NAME:-Llama-SEA-LION-v3-8B-IT}"
    ]
